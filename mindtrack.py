# -*- coding: utf-8 -*-
"""MINDTRACK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HOuYG-4qgj6V0TjgSwlu4N6uYQZUbKol
"""

import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow import keras
from sklearn.model_selection import train_test_split
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Concatenate, Dropout
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from scipy import stats
import pickle
import random
from google.colab import drive
drive.mount('/content/drive')

# Set random seeds for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Feature extraction functions
def extract_stroke_consistency(img):
    """Measure stroke consistency based on intensity variations"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) > 2 else img

    # Apply Gaussian blur to reduce noise
    blurred = cv2.GaussianBlur(gray, (5, 5), 0)

    # Apply Canny edge detection
    edges = cv2.Canny(blurred, 50, 150)

    # Calculate standard deviation of edge intensity as a measure of consistency
    if np.sum(edges) > 0:  # Check if there are any edges detected
        return np.std(edges[edges > 0]) / 255.0  # Normalize
    return 0.0

def extract_letter_spacing(img):
    """Measure letter spacing using horizontal projection"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) > 2 else img

    # Threshold to binary image
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Horizontal projection (sum of white pixels in each column)
    h_proj = np.sum(binary, axis=0) / 255.0

    # Calculate spacing metric (standard deviation of distances between peaks)
    peaks = []
    for i in range(1, len(h_proj) - 1):
        if h_proj[i] > h_proj[i-1] and h_proj[i] > h_proj[i+1] and h_proj[i] > 5:
            peaks.append(i)

    if len(peaks) > 1:
        distances = [peaks[i+1] - peaks[i] for i in range(len(peaks) - 1)]
        return np.std(distances) / img.shape[1]  # Normalize by image width
    return 0.0

def extract_alignment(img):
    """Measure alignment of text lines"""
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) > 2 else img

    # Threshold to binary image
    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Horizontal projection (sum of white pixels in each row)
    v_proj = np.sum(binary, axis=1) / 255.0

    # Find rows with text (where projection > threshold)
    text_rows = [i for i, val in enumerate(v_proj) if val > binary.shape[1] * 0.1]

    if text_rows:
        # Split into continuous line segments
        lines = []
        current_line = [text_rows[0]]

        for i in range(1, len(text_rows)):
            if text_rows[i] - text_rows[i-1] <= 2:  # If rows are adjacent or close
                current_line.append(text_rows[i])
            else:
                if len(current_line) > 5:  # Minimum line length
                    lines.append(current_line)
                current_line = [text_rows[i]]

        if len(current_line) > 5:
            lines.append(current_line)

        # Calculate left margin variation
        margins = []
        for line in lines:
            line_img = binary[min(line):max(line), :]
            for col in range(line_img.shape[1]):
                if np.sum(line_img[:, col]) > 0:
                    margins.append(col)
                    break

        if len(margins) > 1:
            return np.std(margins) / img.shape[1]  # Normalize by image width

    return 0.0

def extract_features(image):
    """Extract multiple handwriting features from an image"""
    features = {
        'stroke_consistency': extract_stroke_consistency(image),
        'letter_spacing': extract_letter_spacing(image),
        'alignment': extract_alignment(image)
    }

    # More feature extraction can be added here

    # Create a fixed-length feature vector
    feature_vector = [
        features['stroke_consistency'],
        features['letter_spacing'],
        features['alignment'],
        # Add more features as needed to reach desired length (padding with zeros)
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
    ]

    return feature_vector[:10]  # Return first 10 features

def preprocess_images(data_dir, img_size):
    """Process all images and extract features"""
    processed_dir = os.path.join(data_dir, 'processed')
    features_file = os.path.join(data_dir, 'handwriting_features.pkl')

    # Create processed directory if it doesn't exist
    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)

        # Process each image in the dataset
        features_dict = {}

        # For each class directory
        for class_name in os.listdir(data_dir):
            class_dir = os.path.join(data_dir, class_name)

            # Skip if not a directory or is the processed directory
            if not os.path.isdir(class_dir) or class_name == 'processed':
                continue

            # Create class directory in processed folder
            processed_class_dir = os.path.join(processed_dir, class_name)
            if not os.path.exists(processed_class_dir):
                os.makedirs(processed_class_dir)

            # Process each image in the class directory
            for img_name in os.listdir(class_dir):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(class_dir, img_name)

                    # Read and resize image
                    img = cv2.imread(img_path)
                    if img is not None:
                        resized_img = cv2.resize(img, img_size)

                        # Save processed image
                        processed_img_path = os.path.join(processed_class_dir, img_name)
                        cv2.imwrite(processed_img_path, resized_img)

                        # Extract features
                        features = extract_features(resized_img)

                        # Store features with relative path as key
                        feature_key = os.path.join(class_name, img_name)
                        features_dict[feature_key] = features

        # Save features dictionary
        with open(features_file, 'wb') as f:
            pickle.dump(features_dict, f)

        print(f"Processed images and extracted features for {len(features_dict)} images")

    else:
        # Load existing features
        if os.path.exists(features_file):
            with open(features_file, 'rb') as f:
                features_dict = pickle.load(f)
            print(f"Loaded existing features for {len(features_dict)} images")
        else:
            features_dict = {}
            print("No existing features found")

    return processed_dir, features_file

# Create CustomDataGenerator for combined features
class CustomDataGenerator(keras.utils.Sequence):
    def __init__(self, directory=None, batch_size=32, img_size=(224, 224),
                 features_dict=None, class_indices=None, class_mode='binary',
                 subset=None, image_paths=None, labels=None, **kwargs):
        # Call parent class init
        super().__init__(**kwargs)

        self.directory = directory
        self.batch_size = batch_size
        self.img_size = img_size
        self.features_dict = features_dict or {}
        self.class_indices = class_indices
        self.class_mode = class_mode
        self.image_paths = []
        self.labels = []

        # If image_paths and labels are provided directly, use them
        if image_paths is not None and labels is not None:
            self.image_paths = np.array(image_paths)
            self.labels = np.array(labels)
        # Otherwise, scan directory for images
        elif directory:
            for class_name in os.listdir(directory):
                class_path = os.path.join(directory, class_name)
                if os.path.isdir(class_path):
                    class_label = self.class_indices[class_name]
                    for img_name in os.listdir(class_path):
                        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                            img_path = os.path.join(class_path, img_name)
                            feature_key = os.path.join(class_name, img_name)

                            # Only add images with extracted features if features_dict provided
                            if not features_dict or feature_key in features_dict:
                                self.image_paths.append(img_path)
                                self.labels.append(class_label)

            # Convert to numpy arrays
            self.image_paths = np.array(self.image_paths)
            self.labels = np.array(self.labels)

        # Print debug info
        print(f"Created generator with {len(self.image_paths)} images")

        # Shuffle data
        if len(self.image_paths) > 0:
            indices = np.arange(len(self.image_paths))
            np.random.shuffle(indices)
            self.image_paths = self.image_paths[indices]
            self.labels = self.labels[indices]

        # Split into training and validation if needed
        if subset and len(self.image_paths) > 0:
            split_idx = int(len(self.image_paths) * 0.8)  # 80% for training
            if subset == 'training':
                self.image_paths = self.image_paths[:split_idx]
                self.labels = self.labels[:split_idx]
            elif subset == 'validation':
                self.image_paths = self.image_paths[split_idx:]
                self.labels = self.labels[split_idx:]

    def __len__(self):
        """Return the number of batches per epoch"""
        return max(1, int(np.ceil(len(self.image_paths) / self.batch_size)))

    @property
    def num_batches(self):
        """Required property for Keras training"""
        return self.__len__()

    def __getitem__(self, idx):
        # Safety check for empty batches
        if len(self.image_paths) == 0:
            # Return empty batch with correct shapes
            empty_images = np.zeros((1, self.img_size[0], self.img_size[1], 3), dtype=np.float32)
            empty_features = np.zeros((1, 10), dtype=np.float32)
            empty_labels = np.zeros(1, dtype=np.int32)
            return {"image_input": empty_images, "feature_input": empty_features}, empty_labels

        # Get batch indices with bounds checking
        start_idx = idx * self.batch_size
        end_idx = min(start_idx + self.batch_size, len(self.image_paths))
        batch_paths = self.image_paths[start_idx:end_idx]
        batch_labels = self.labels[start_idx:end_idx]

        # Initialize batch arrays
        batch_size = len(batch_paths)
        batch_images = np.zeros((batch_size, self.img_size[0], self.img_size[1], 3), dtype=np.float32)
        batch_features = np.zeros((batch_size, 10), dtype=np.float32)

        for i, img_path in enumerate(batch_paths):
            try:
                # Load and preprocess image
                img = cv2.imread(img_path)
                if img is None:
                    print(f"Warning: Could not load image {img_path}")
                    img = np.zeros((self.img_size[0], self.img_size[1], 3))
                img = cv2.resize(img, self.img_size)
                img = img / 255.0  # Normalize
                batch_images[i] = img

                # Get features if available
                if self.features_dict:
                    rel_path = os.path.relpath(img_path, self.directory) if self.directory else os.path.basename(img_path)
                    feature_key = os.path.join(os.path.dirname(rel_path), os.path.basename(rel_path))
                    batch_features[i] = np.array(self.features_dict.get(feature_key, np.zeros(10)))
            except Exception as e:
                print(f"Error processing image {img_path}: {e}")
                # Use zeros for this sample
                batch_images[i] = np.zeros((self.img_size[0], self.img_size[1], 3))
                batch_features[i] = np.zeros(10)

        # Normalize features if we have any
        if batch_size > 0 and np.any(batch_features):
            try:
                batch_features = stats.zscore(batch_features, axis=0, nan_policy='omit')
                batch_features = np.nan_to_num(batch_features)  # Replace NaNs with zeros
            except Exception as e:
                print(f"Error normalizing features: {e}")

        # Return in the format TensorFlow expects
        return {"image_input": batch_images, "feature_input": batch_features}, batch_labels

    @property
    def classes(self):
        return self.labels

    def on_epoch_end(self):
        """Method called at the end of every epoch"""
        indices = np.arange(len(self.image_paths))
        np.random.shuffle(indices)
        self.image_paths = self.image_paths[indices]
        self.labels = self.labels[indices]

def build_model(img_size=(224, 224), num_features=10):
    """Build a model that combines CNN features with handwriting features"""
    # Input for images
    img_input = Input(shape=(img_size[0], img_size[1], 3), name='image_input')

    # Use MobileNetV2 for image feature extraction
    base_model = MobileNetV2(input_tensor=img_input, include_top=False, weights='imagenet')

    # Freeze base model layers
    for layer in base_model.layers:
        layer.trainable = False

    # Get CNN features
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)

    # Input for handwriting features
    feature_input = Input(shape=(num_features,), name='feature_input')

    # Process handwriting features
    y = Dense(32, activation='relu')(feature_input)
    y = Dropout(0.3)(y)

    # Combine both feature sets
    combined = Concatenate()([x, y])

    # Output layer
    output = Dense(1, activation='sigmoid')(combined)

    # Create and compile model
    model = Model(inputs=[img_input, feature_input], outputs=output)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

def train_model(model, data_dir, epochs=10, batch_size=32, img_size=(224, 224)):
    """Train the model using data generator"""
    # Process data directory to get features
    processed_dir, features_file = preprocess_images(data_dir, img_size)

    # Load features dictionary
    features_dict = {}
    if os.path.exists(features_file):
        with open(features_file, 'rb') as f:
            features_dict = pickle.load(f)
        print(f"Loaded features for {len(features_dict)} images")

    # Get class names
    class_names = sorted(os.listdir(processed_dir))
    class_indices = {name: idx for idx, name in enumerate(class_names)}

    # Create training and validation generators
    train_gen = CustomDataGenerator(
        directory=processed_dir,
        batch_size=batch_size,
        img_size=img_size,
        features_dict=features_dict,
        class_indices=class_indices,
        subset='training'
    )

    val_gen = CustomDataGenerator(
        directory=processed_dir,
        batch_size=batch_size,
        img_size=img_size,
        features_dict=features_dict,
        class_indices=class_indices,
        subset='validation'
    )

    # Define callbacks
    callbacks = [
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)
    ]

    # Train the model
    print(f"Starting training with {len(train_gen)} training batches and {len(val_gen)} validation batches")
    history = model.fit(
        train_gen,
        epochs=epochs,
        validation_data=val_gen,
        callbacks=callbacks
    )

    return model, history

def evaluate_model(model, data_dir, img_size=(224, 224), batch_size=32):
    """Evaluate the model on test data"""
    # Process data directory to get features
    processed_dir, features_file = preprocess_images(data_dir, img_size)

    # Load features dictionary
    features_dict = {}
    if os.path.exists(features_file):
        with open(features_file, 'rb') as f:
            features_dict = pickle.load(f)

    # Get class names
    class_names = sorted(os.listdir(processed_dir))
    class_indices = {name: idx for idx, name in enumerate(class_names)}

    # Create test generator
    test_gen = CustomDataGenerator(
        directory=processed_dir,
        batch_size=batch_size,
        img_size=img_size,
        features_dict=features_dict,
        class_indices=class_indices
    )

    # Evaluate model
    results = model.evaluate(test_gen)

    # Print results
    print(f"Test Loss: {results[0]:.4f}")
    print(f"Test Accuracy: {results[1]:.4f}")

    # Generate predictions for confusion matrix
    all_preds = []
    all_labels = []

    for i in range(len(test_gen)):
        inputs, labels = test_gen[i]
        preds = model.predict(inputs)
        preds_binary = (preds > 0.5).astype(int).flatten()

        all_preds.extend(preds_binary)
        all_labels.extend(labels)

    # Create confusion matrix
    from sklearn.metrics import confusion_matrix, classification_report
    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names, rotation=45)
    plt.yticks(tick_marks, class_names)

    # Add text annotations
    thresh = cm.max() / 2
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            plt.text(j, i, cm[i, j],
                    horizontalalignment="center",
                    color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

    # Print classification report
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds, target_names=class_names))

    return cm, all_preds, all_labels

def analyze_feature_importance(model, data_dir, img_size=(224, 224), batch_size=32):
    """Analyze feature importance by comparing performance with and without features"""
    # Process data directory to get features
    processed_dir, features_file = preprocess_images(data_dir, img_size)

    # Load features dictionary
    with open(features_file, 'rb') as f:
        features_dict = pickle.load(f)

    # Get class names
    class_names = sorted(os.listdir(processed_dir))
    class_indices = {name: idx for idx, name in enumerate(class_names)}

    # Create test generator with all features
    test_gen = CustomDataGenerator(
        directory=processed_dir,
        batch_size=batch_size,
        img_size=img_size,
        features_dict=features_dict,
        class_indices=class_indices
    )

    # Evaluate model with all features
    results_with_features = model.evaluate(test_gen)
    print(f"Performance with all features - Loss: {results_with_features[0]:.4f}, Accuracy: {results_with_features[1]:.4f}")

    # Create a dictionary to store importance of each feature
    feature_importance = {}
    original_accuracy = results_with_features[1]

    # For each feature, zero it out and see how it affects performance
    for feature_idx in range(10):
        # Create a modified features dictionary
        modified_features = {}
        for key, value in features_dict.items():
            modified_value = value.copy()
            modified_value[feature_idx] = 0.0
            modified_features[key] = modified_value

        # Create test generator with modified features
        test_gen_modified = CustomDataGenerator(
            directory=processed_dir,
            batch_size=batch_size,
            img_size=img_size,
            features_dict=modified_features,
            class_indices=class_indices
        )

        # Evaluate model with modified features
        results_modified = model.evaluate(test_gen_modified)
        accuracy_drop = original_accuracy - results_modified[1]

        # Store feature importance
        feature_importance[f"Feature {feature_idx}"] = accuracy_drop

    # Sort features by importance
    sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    features = [x[0] for x in sorted_importance]
    importance = [x[1] for x in sorted_importance]
    plt.bar(features, importance)
    plt.xlabel('Feature')
    plt.ylabel('Importance (Accuracy Drop)')
    plt.title('Feature Importance')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return feature_importance

def save_model(model, features_dict, model_path='dysgraphia_model.keras', features_path='features_dict.pkl'):
    """Save the model and features dictionary"""
    # Save model
    model.save(model_path)

    # Save features dictionary
    with open(features_path, 'wb') as f:
        pickle.dump(features_dict, f)

    print(f"Model saved to {model_path}")
    print(f"Features dictionary saved to {features_path}")

def load_model(model_path='dysgraphia_model.keras', features_path='features_dict.pkl'):
    """Load the model and features dictionary"""
    # Load model
    model = keras.models.load_model(model_path)

    # Load features dictionary
    with open(features_path, 'rb') as f:
        features_dict = pickle.load(f)

    return model, features_dict

def predict_single_image(model, image_path, features_dict=None, img_size=(224, 224)):
    """Predict dysgraphia for a single image"""
    # Load and preprocess image
    img = cv2.imread(image_path)
    img = cv2.resize(img, img_size)
    img_normalized = img / 255.0

    # Extract features if no features dictionary provided
    if features_dict is None:
        features = extract_features(img)
    else:
        # Try to find the image in the features dictionary
        img_name = os.path.basename(image_path)
        if img_name in features_dict:
            features = features_dict[img_name]
        else:
            features = extract_features(img)

    # Normalize features
    features = np.array(features).reshape(1, -1)
    features = stats.zscore(features, axis=0, nan_policy='omit')
    features = np.nan_to_num(features)

    # Prepare input for model
    img_input = np.expand_dims(img_normalized, axis=0)

    # Make prediction
    prediction = model.predict({"image_input": img_input, "feature_input": features})

    # Get binary prediction
    has_dysgraphia = bool(prediction[0][0] > 0.5)

    # Return result
    result = {
        "has_dysgraphia": has_dysgraphia,
        "confidence": float(prediction[0][0]),
        "features": features.flatten().tolist()
    }

    return result

# Main execution - modify this for your specific use case
if __name__ == "__main__":
    # Set parameters
    data_dir = "/content/drive/MyDrive/Handwriting"  # Change this to your actual dataset path
    img_size = (224, 224)
    batch_size = 16  # Smaller batch size if dataset is small
    epochs = 20

    # Build enhanced model
    model = build_model(img_size=img_size, num_features=10)
    print("Enhanced model built successfully")

    # Train model
    model, history = train_model(model, data_dir, epochs=epochs, batch_size=batch_size, img_size=img_size)
    print("Training complete")

    # Evaluate model
    evaluate_model(model, data_dir, img_size=img_size, batch_size=batch_size)

    # Analyze feature importance
    analyze_feature_importance(model, data_dir, img_size=img_size, batch_size=batch_size)

    # Save model and features
    processed_dir, features_file = preprocess_images(data_dir, img_size)
    with open(features_file, 'rb') as f:
        features_dict = pickle.load(f)
    save_model(model, features_dict, model_path='dysgraphia_model.keras', features_path='features_dict.pkl')

# Plot training history (use this after training is complete)
def plot_history(history):
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='lower right')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper right')

    plt.tight_layout()
    plt.show()

import numpy as np
import cv2
import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model("dysgraphia_model.keras")

def preprocess_image(image_path):
    img = cv2.imread(image_path)  # Read image
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB
    img = cv2.resize(img, (224, 224))  # Resize to 224x224
    img = img / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)  # Reshape to (1, 224, 224, 3)
    return img

def predict_dysgraphia(image_path):
    processed_image = preprocess_image(image_path)
    dummy_features = np.zeros((1, 10))  # Dummy feature input

    prediction = model.predict([processed_image, dummy_features])
    raw_value = prediction[0][0]  # Extract the raw sigmoid output

    print(f"Raw Prediction Output: {raw_value}")  # Debugging output

    # Adjust threshold if needed
    label = "Dysgraphic" if raw_value < 0.5 else "Non-dysgraphic"  # Flipped threshold
    confidence = 1 - raw_value if label == "Dysgraphic" else raw_value

    return label, confidence

# Example usage
image_path = "/content/drive/MyDrive/Handwriting/Dysgraphic/27400.jpeg"
result, confidence = predict_dysgraphia(image_path)
print(f"Prediction: {result} (Confidence: {confidence:.2%})")

model_save_path = "/content/drive/MyDrive/dysgraphia_model.keras"
model.save(model_save_path)
print(f"Model saved to: {model_save_path}")

